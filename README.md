# Vision-LiDAR-Sensor-Colaboration

# Vision-LiDAR Fusion 알고리즘 문서

> Vision(카메라)과 LiDAR 센서를 결합하여 객체를 추적하는 시스템의 알고리즘 설명

---

## 목차

1. [시스템 개요](#1-시스템-개요)
2. [Vision (카메라) 처리](#2-vision-카메라-처리)
3. [LiDAR 처리](#3-lidar-처리)
4. [Fusion (센서 융합)](#4-fusion-센서-융합)
5. [주요 파라미터](#5-주요-파라미터)

---

## 1. 시스템 개요

### 1.1 전체 구조

```
┌─────────────────────────────────────────────────────────────┐
│                    Vision-LiDAR Fusion                      │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   ┌─────────────┐              ┌─────────────┐             │
│   │   카메라     │              │   LiDAR     │             │
│   │  (Vision)   │              │  (2D 스캔)   │             │
│   └──────┬──────┘              └──────┬──────┘             │
│          │                            │                     │
│          ▼                            ▼                     │
│   ┌─────────────┐              ┌─────────────┐             │
│   │    YOLO     │              │  클러스터링  │             │
│   │  (객체검출)  │              │  (점군→객체) │             │
│   └──────┬──────┘              └──────┬──────┘             │
│          │                            │                     │
│          ▼                            ▼                     │
│   ┌─────────────┐              ┌─────────────┐             │
│   │  ByteTrack  │              │  위치 기반   │             │
│   │  (ID 추적)  │              │   추적      │             │
│   └──────┬──────┘              └──────┬──────┘             │
│          │                            │                     │
│          └────────────┬───────────────┘                     │
│                       ▼                                     │
│               ┌─────────────┐                               │
│               │   Fusion    │                               │
│               │  (센서융합)  │                               │
│               └─────────────┘                               │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 1.2 각 센서의 역할

| 센서 | 장점 | 단점 | 역할 |
|------|------|------|------|
| **카메라** | 객체 식별 가능 (사람/차 구분) | 거리 측정 불가 | "무엇"인지 판단 |
| **LiDAR** | 정확한 거리 측정 | 객체 종류 구분 어려움 | "어디"있는지 판단 |

**Fusion = "무엇"이 "어디"에 있는지 결합**

---

## 2. Vision (카메라) 처리

### 2.1 처리 흐름

```
카메라 영상 → YOLO (객체 검출) → ByteTrack (ID 추적) → 검출 결과
```

### 2.2 YOLO - 객체 검출

**역할**: 영상에서 "사람"을 찾아 박스(Bounding Box)로 표시

```
입력: 카메라 프레임 (1280x720)
       ┌────────────────────┐
       │    ┌───┐           │
       │    │사람│           │
       │    └───┘           │
       └────────────────────┘
                ↓
출력: 박스 좌표 + 신뢰도
       - bbox: (x1, y1, x2, y2)
       - confidence: 0.85
       - class: "person"
```

### 2.3 ByteTrack - ID 추적

**역할**: 프레임마다 검출된 사람에게 **고유 ID**를 부여하고 유지

```
Frame 1          Frame 2          Frame 3
┌───┐            ┌───┐            ┌───┐
│ID1│ ────────→  │ID1│ ────────→  │ID1│   같은 사람 = 같은 ID
└───┘            └───┘            └───┘
```

**왜 필요한가?**
- YOLO는 매 프레임 독립적으로 검출 → "같은 사람"인지 모름
- ByteTrack이 위치/움직임 기반으로 같은 사람을 연결

### 2.4 각도 계산

카메라 중앙 기준으로 객체의 **방향(각도)**을 계산:

```
카메라 화면:
    ←────── 1280px ──────→
    ┌────────────────────┐
    │         │          │
    │    ┌───┐│          │   객체가 왼쪽에 있음
    │    │   ││          │   → 각도 = -15°
    │    └───┘│          │
    │         │          │
    └────────────────────┘
           중앙(0°)

계산식: 각도 = (객체중심 - 화면중심) / 화면너비 × FOV
```

---

## 3. LiDAR 처리

### 3.1 처리 흐름

```
LiDAR 스캔 → 전처리 → 클러스터링 → 후보 필터링 → 추적
   (거리배열)   (좌표변환)   (점→덩어리)    (크기필터)   (ID부여)
```

### 3.2 LiDAR 데이터 구조

2D LiDAR는 회전하며 주변 거리를 측정:

```
        전방 (180°)
            │
   270° ────┼──── 90°
            │
        후방 (0°)

출력: ranges[] = [거리1, 거리2, ..., 거리N]
      각 인덱스가 특정 각도에 해당
```

### 3.3 전처리 (Preprocessing)

**1단계: 유효 데이터 필터링**
```python
# 너무 가깝거나 먼 데이터 제거
if 거리 < 0.2m or 거리 > 6.0m:
    제외

# 관심 영역(ROI) 외 제거
if 각도가 전방 ±90° 범위 밖:
    제외
```

**2단계: 좌표 변환 (극좌표 → 직교좌표)**
```
극좌표: (거리, 각도) = (2m, 30°)
    ↓
직교좌표: (x, y) = (1.73m, 1.0m)
```

**3단계: 각도 순서 정렬** ⚠️ 중요!
```
문제: LiDAR 배열이 -180°→+180° 순서인데,
      ROI가 180° 주변이면 배열의 시작과 끝에 분산됨

      배열: [-180°...선택됨...][-90°...제외...][+90°...선택됨...][+180°]
                   ↑                                    ↑
               배열 앞쪽                            배열 뒷쪽

해결: 각도 기준으로 재정렬
      → 연속된 물체가 연속된 데이터로 유지됨
```

### 3.4 클러스터링 (Clustering)

**목적**: 개별 점(point)들을 "덩어리(cluster)"로 묶기

```
LiDAR 점군:          클러스터링 후:
    ● ●                 ┌─────┐
    ● ●        →       │물체1│
    ●                   └─────┘

      ● ●               ┌─────┐
      ● ●      →       │물체2│
                        └─────┘
```

**알고리즘: 거리 적응형 클러스터링**

```python
임계값 = a × 거리 + b

# 가까운 물체: 임계값 작음 → 정밀 분리
# 먼 물체: 임계값 큼 → LiDAR 해상도 고려
```

```
예시 (a=0.2, b=0.3):
- 거리 1m → 임계값 0.5m
- 거리 3m → 임계값 0.9m

인접 점 사이 거리 < 임계값 → 같은 클러스터
인접 점 사이 거리 > 임계값 → 다른 클러스터
```

### 3.5 후보 필터링

사람 크기에 맞는 클러스터만 선택:

```python
# 크기 필터
if 클러스터_폭 < 0.1m or 클러스터_폭 > 1.5m:
    제외  # 너무 작거나 큰 것은 사람이 아님

# 포인트 수 필터
if 포인트_수 < 5 or 포인트_수 > 200:
    제외  # 노이즈이거나 벽 같은 큰 물체
```

### 3.6 추적 (Tracking)

**목적**: 매 프레임 클러스터에 **고유 ID**를 부여하고 유지

**알고리즘: Greedy Nearest Neighbor (탐욕적 최근접 이웃)**

```
Frame N의 트랙들         Frame N+1의 클러스터들
┌─────────┐              ┌─────────┐
│ Track 1 │ ←─ 0.15m ──→ │Cluster A│  ← 매칭! (가장 가까움)
└─────────┘              └─────────┘

┌─────────┐              ┌─────────┐
│ Track 2 │ ←─ 0.8m ───→ │Cluster B│  ← 매칭 실패 (너무 멈)
└─────────┘              └─────────┘
```

**매칭 규칙:**
```python
max_move = 0.3m  # 한 프레임에 최대 이동 가능 거리

if 거리 < max_move:
    매칭 성공 → 같은 ID 유지
else:
    매칭 실패 → ID 끊김
```

**트랙 생명주기:**

| 상태 | 조건 | 설명 |
|------|------|------|
| 생성 | 새 클러스터 감지 | 새 Track ID 부여 |
| 확정 | 3프레임 연속 매칭 | 신뢰할 수 있는 트랙 |
| 유지 | 매칭 성공 | ID 계속 유지 |
| 삭제 | 5프레임 연속 미매칭 | 사라진 것으로 판단 |

---

## 4. Fusion (센서 융합)

### 4.1 Fusion의 목적

```
Vision만: "저기 사람이 있다" (방향만 알고, 거리 모름)
LiDAR만: "저기 뭔가 있다" (거리는 알지만, 사람인지 모름)
Fusion:  "저기 사람이 2.5m 거리에 있다" (방향 + 거리 + 종류)
```

### 4.2 매칭 방식

Vision 각도와 LiDAR 각도를 비교하여 같은 객체를 찾음:

```
        Vision 각도: -10°
             ↓
    ┌────────────────────┐
    │        │ ●         │  카메라 화면
    │        │사람        │
    └────────────────────┘

        LiDAR 각도: -12°
             ↓
    ─────────●───────────  LiDAR 탑뷰
            2.5m

    각도 차이 = |-10° - (-12°)| = 2° < 허용오차(15°)
    → 매칭 성공!
```

### 4.3 튐 감지 및 재탐색

**문제**: LiDAR 트랙이 갑자기 다른 위치로 "튀는" 현상
- 원인: 오클루전, 반사 오류, 다른 객체로 전환 등

**해결: Vision 기반 재탐색**

```
정상 상태:
  Vision ID=5 ←→ LiDAR ID=3  (매칭 유지)

튐 발생 (LiDAR가 0.5m 이상 점프):
  Vision ID=5 ←→ LiDAR ID=3  → 튐 감지!
                     ↓
  Vision ID=5 ←→ LiDAR ID=?  → LiDAR 무효화
                     ↓
  Vision 각도 기준으로 새 LiDAR 검색
                     ↓
  Vision ID=5 ←→ LiDAR ID=7  → 재매칭 성공
```

**파라미터:**
```python
max_move = 0.3m       # 정상 이동 범위
jump_threshold = 0.5m # 이 이상 이동 = "튐"으로 판단
```

### 4.4 Fusion 상태

| 상태 | 설명 |
|------|------|
| `MATCHING` | Vision + LiDAR 모두 추적 중 |
| `SEARCHING` | LiDAR를 찾는 중 |
| `VISION_ONLY` | LiDAR를 찾지 못해 Vision만 사용 |

---

## 5. 주요 파라미터

### 5.1 Vision 파라미터

| 파라미터 | 기본값 | 설명 |
|---------|--------|------|
| `conf` | 0.25 | YOLO 신뢰도 임계값 |
| `camera_fov` | 60° | 카메라 수평 화각 |

### 5.2 LiDAR 파라미터

| 파라미터 | 기본값 | 설명 |
|---------|--------|------|
| `range_min` | 0.2m | 최소 감지 거리 |
| `range_max` | 6.0m | 최대 감지 거리 |
| `roi_angle_deg` | 90° | 관심 영역 각도 (±90°) |
| `cluster_a` | 0.2 | 클러스터링 거리 계수 |
| `cluster_b` | 0.3 | 클러스터링 기본 임계값 |
| `width_min` | 0.1m | 최소 클러스터 폭 |
| `width_max` | 1.5m | 최대 클러스터 폭 |
| `min_points` | 5 | 최소 포인트 수 |

### 5.3 추적 파라미터

| 파라미터 | 기본값 | 설명 |
|---------|--------|------|
| `max_move` | 0.3m | 프레임당 최대 이동 거리 |
| `jump_threshold` | 0.5m | 튐 감지 임계값 |
| `hit_min` | 3 | 트랙 확정까지 필요한 연속 매칭 |
| `miss_max` | 5 | 트랙 삭제까지 허용하는 미매칭 |

### 5.4 Fusion 파라미터

| 파라미터 | 기본값 | 설명 |
|---------|--------|------|
| `fusion_angle_tolerance` | 0.25 rad (~14°) | 각도 매칭 허용 오차 |
| `fusion_distance_tolerance` | 1.0m | 거리 매칭 허용 오차 |

---

## 부록: 해결된 버그

### A. 정면 물체 분리 문제

**증상**: 가까운 정면 물체가 두 개의 클러스터로 분리됨

**원인**: LiDAR 데이터가 배열 인덱스 순서로 처리되어, ROI 경계에서 포인트 순서가 불연속

**해결**: 전처리 후 각도 순서로 정렬

```python
# 수정 전: 배열 순서대로 추가
for i, r in enumerate(ranges):
    points_xy.append(...)

# 수정 후: 각도 순서로 정렬
points_data.sort(key=lambda x: x[0])  # theta 기준 정렬
```

---

## 참고

- YOLO: [Ultralytics YOLO](https://github.com/ultralytics/ultralytics)
- ByteTrack: [ByteTrack Paper](https://arxiv.org/abs/2110.06864)
- ROS2 LaserScan: [sensor_msgs/LaserScan](http://docs.ros.org/en/noetic/api/sensor_msgs/html/msg/LaserScan.html)
